{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Zambia Gazette Text Classification Project\n","\n","\n","\n","## Table of Contents\n","1. [Business Understanding](#1-business-understanding)  \n","2. [Methodology](#2-methodology)  \n","3. [Tools and Technologies](#3-tools-and-technologies)  \n","4. [Expected Outcomes](#4-expected-outcomes)  \n","5. [Future Enhancements](#5-future-enhancements)\n","\n","\n","# 1. BUSINESS UNDERSTANDING\n","## 1.1 PROBLEM STATEMENT\n","\n","The Zambia Government Gazette publishes official notices, legal updates, tenders, and public announcements. These documents are published in PDF format, and often unstructured, making it difficult for stakeholders such as lawyers, journalists, researchers, and the general public to quickly find relevant information. Manual searching is time-consuming and prone to oversight. There is a need for an automated method to classify Gazette notices into categories (e.g., legal notices, tenders, appointments, public warnings) for faster retrieval and analysis.\n","\n","## 1.2 BUSINESS OBJECTIVES\n","\n","The primary objective is to develop a system that automatically processes and classifies Gazette publications into predefined categories. Success will mean that end users can:\n","\n","Quickly identify and filter notices by category.\n","\n","Reduce time spent manually scanning through documents.\n","\n","Gain improved access to relevant legal or public information.\n","\n","From a real-world perspective, this will increase efficiency for professionals and citizens who rely on the Gazette for important updates.\n","\n","## 1.3 DATA MINING GOALS\n","\n","We will build a text classification model that:\n","\n","Extracts text from Gazette PDFs.\n","\n","Preprocesses the text (cleaning, tokenization, stopword removal).\n","\n","Classifies each notice into categories.\n","\n","Outputs labeled data for easy search and retrieval.\n","\n","The approach will likely involve Natural Language Processing (NLP) and machine learning algorithms such as Logistic Regression or Support Vector Machines.\n","\n","## 1.4 INITIAL SUCCESS CRITERIA\n","\n","The project will be considered successful if:\n","\n","The classification model achieves at least 80% accuracy on the test dataset.\n","\n","Categories are clearly and vividly defined, distinct, and interpretable.\n","\n","The pipeline can handle at least 10 new Gazette PDFs per month without major manual intervention.\n","\n","Users confirm that classification results improve search speed and relevance compared to manual reading.\n","\n","## 1.5 SCOPE & ASSUMPTIONS\n","\n","Scope:\n","\n","Focus on classifying Gazette notices into 4â€“6 main categories.\n","\n","Work only with English-language Gazettes.\n","\n","Process and analyze a subset of recent Gazette issues.\n","\n","Assumptions:\n","\n","PDF files are accessible and legally permissible for analysis.\n","\n","Categories remain consistent over time.\n","\n","OCR (Optical Character Recognition) will be needed for scanned documents.\n","\n","## 1.6 RISKS & CONSTRAINTS\n","\n","Risks:\n","\n","Poor text quality from scanned PDFs may reduce OCR accuracy.\n","\n","Some notices may belong to multiple categories, complicating classification.\n","\n","Limited labelled training data could impact model performance.\n","\n","Constraints:\n","\n","Legal constraint: Must comply with any copyright or government data use regulations.\n","\n","## 1.7 Expected Benefits\n","The system will enable faster retrieval of Gazette notices, enhance transparency, and support informed decision-making for both professionals and the public.\n","\n","## 1.8 METHODOLOGY\n","### 1.8.1 DATA COLLECTION\n","\n","Download Gazette PDFs from the official Zambia Government Gazette website.\n","\n","Ensure documents cover a representative period to include diverse categories.\n","\n","Maintain a record of file metadata (date, publication number) for reference.\n","### 1.8.2 DATA PREPROCESSING\n","\n","Convert PDFs to text using OCR for scanned documents.\n","\n","Remove irrelevant elements (headers, footers, page numbers).\n","\n","Tokenize text and remove stopwords, punctuation, and special characters.\n","\n","Standardize text formatting (e.g., lowercasing, stemming).\n","\n","### 1.8.3 MODEL SELECTION\n","\n","Evaluate multiple classification algorithms: Logistic Regression, Support Vector Machines, and Random Forest.\n","\n","Use TF-IDF or word embeddings to represent text features.\n","\n","Optimize model parameters using cross-validation.\n","\n","### 1.8.4 EVALUATION METRICS\n","\n","Accuracy, precision, recall, and F1-score for each category.\n","\n","Confusion matrix to identify misclassification trends.\n","\n","User feedback on relevance and usefulness of classified notices.\n","\n","## 1.9 TOOLS AND TECHNOLOGIES\n","\n","Programming Languages: Python\n","\n","Libraries: scikit-learn, pandas, NumPy, NLTK, spaCy, PyPDF2, Tesseract OCR\n","\n","Environment: Jupyter Notebook / Python IDE\n","\n","Version Control: Git / GitHub\n","\n","## 1.10 EXPECTED OUTCOMES\n","\n","Automated classification of Gazette notices into predefined categories.\n","\n","A searchable dataset with labeled notices for faster retrieval.\n","\n","Insights into the distribution and frequency of notice types.\n","\n","Reduced manual effort for users accessing Gazette information.\n","\n","## 1.11 FUTURE ENHANCEMENTS\n","\n","Implement a web interface for searching and filtering classified notices.\n","\n","Incorporate advanced NLP techniques like BERT for improved classification accuracy.\n","\n","Expand to multilingual Gazettes or other official publications.\n","\n","Introduce trend analysis and reporting for frequently published notice types."],"metadata":{"id":"C9pYZYBidvZH"}},{"cell_type":"code","source":["# Mount Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":321},"id":"Vt0gYyR5sn_Y","executionInfo":{"status":"error","timestamp":1755528752708,"user_tz":-120,"elapsed":7212,"user":{"displayName":"Moses Mwale","userId":"06453881690152720812"}},"outputId":"4efd88cd-5fb1-4edb-afa0-11f930b47d4a"},"execution_count":4,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"Mountpoint must not already contain files","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-3793592510.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Mount Google Drive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    197\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Mountpoint must not be a symlink'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Mountpoint must not already contain files'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Mountpoint must either be a directory or not exist'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Mountpoint must not already contain files"]}]},{"cell_type":"code","source":["# Create project folders on Drive (adjust path if different)\n","import os\n","base = \"/content/drive/MyDrive/csc4792-project_team_28\"\n","folders = [\n","    \"01_Business_Understanding\",\n","    \"02_Data_Understanding/raw_pdfs\",\n","    \"02_Data_Understanding/extracted_csv\",\n","    \"03_Data_Preparation\",\n","    \"04_Modeling\",\n","    \"05_Evaluation\",\n","    \"06_Deployment\",\n","    \"Reports\",\n","    \"Slides\"\n","]\n","for f in folders:\n","    os.makedirs(os.path.join(base, f), exist_ok=True)\n","\n","print(\"Folders created at:\", base)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"id":"khxcNEGcbuy4","executionInfo":{"status":"ok","timestamp":1755156517747,"user_tz":-120,"elapsed":45,"user":{"displayName":"Moses Mwale","userId":"06453881690152720812"}},"outputId":"39af4152-51c3-4bd4-eb31-5dcd574326e4"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["//This cell wil be executed as javascript\n","//An alert will pop up when the cell is run\n","alert(\"Welcome to CSC 4792!\");\n"]},"metadata":{}}]},{"cell_type":"markdown","source":["# 2. Data Understanding\n","\n","## 2.1 Loading the Dataset\n","( Loading CSV into Pandas DataFrame.)\n","\n","## 2.2 Initial Exploration\n","- `df.shape`, `df.head()` and explain results.  \n","- `df.info()`, `df.describe(include=\"all\")` and explain results.  \n","\n","## 2.3 Visual Exploration\n","- Create histograms for numerical columns.  \n","- Create bar plots for categorical columns.\n","\n","## 2.4 Summary of Findings\n","- Summary on dataset structure, missing values, distributions, and early insights.\n"],"metadata":{"id":"qknMT0sj27yn"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"E2hmhbmtqo7U","executionInfo":{"status":"ok","timestamp":1755529806888,"user_tz":-120,"elapsed":15360,"user":{"displayName":"Moses Mwale","userId":"06453881690152720812"}},"outputId":"949bf644-5e8e-47c8-ec31-bb8e8bc2a95e"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["# Extract text from Gazette PDFs into a CSV\n","!pip install --quiet PyMuPDF\n","import fitz, os, pandas as pd, glob\n","\n","BASE = \"/content/drive/MyDrive/csc4792-project_team_28\"\n","RAW_DIR = os.path.join(BASE, \"02_Data_Understanding\", \"raw_pdfs\")\n","OUT_DIR = os.path.join(BASE, \"02_Data_Understanding\", \"extracted_csv\")\n","os.makedirs(OUT_DIR, exist_ok=True)\n","\n","def extract_pdf_to_rows(pdf_path):\n","    rows = []\n","    fn = os.path.basename(pdf_path)\n","    doc_id = os.path.splitext(fn)[0]\n","    with fitz.open(pdf_path) as doc:\n","        for pno, page in enumerate(doc, start=1):\n","            text = page.get_text(\"text\")\n","            paras = [t.strip() for t in text.split(\"\\n\\n\") if t.strip()]\n","            for i, para in enumerate(paras, start=1):\n","                wc = len(para.split())\n","                entry_id = f\"{doc_id}_p{pno}_seg{i}\"\n","                rows.append({\n","                    \"entry_id\": entry_id,\n","                    \"doc_id\": doc_id,\n","                    \"filename\": fn,\n","                    \"page\": pno,\n","                    \"para_idx\": i,\n","                    \"text\": para,\n","                    \"word_count\": wc,\n","                    \"char_len\": len(para),\n","                    \"category\": \"\"\n","                })\n","    return rows\n","\n","all_rows = []\n","pdfs = sorted(glob.glob(os.path.join(RAW_DIR, \"*.pdf\")))\n","print(f\"Found {len(pdfs)} PDF(s).\")\n","\n","for pdf in pdfs:\n","    all_rows.extend(extract_pdf_to_rows(pdf))\n","\n","df = pd.DataFrame(all_rows)\n","\n","# Save output\n","out_csv = os.path.join(OUT_DIR, \"gazette_segments1.csv\")\n","df.to_csv(out_csv, index=False)\n","print(\"Saved:\", out_csv, \"Rows:\", len(df))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jGVFlqdaozqd","executionInfo":{"status":"ok","timestamp":1755530154363,"user_tz":-120,"elapsed":5820,"user":{"displayName":"Moses Mwale","userId":"06453881690152720812"}},"outputId":"313ecdcf-59e5-4234-de1f-7a9183dc5ba2"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 10 PDF(s).\n","Saved: /content/drive/MyDrive/csc4792-project_team_28/02_Data_Understanding/extracted_csv/gazette_segments1.csv Rows: 133\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"ymJ9Ut28ru1I"},"execution_count":null,"outputs":[]}]}